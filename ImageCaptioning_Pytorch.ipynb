{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7a2eb1c4-867c-4716-84c6-ad7365d50d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fbb4c6e8-13a0-4013-ae08-b75b55cd20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from data import ImageCaptionDataset, generate_batch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f322617a-9d51-4dec-a7ec-c3330d7aab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "imgs_path = \"data/img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9dbed6ab-413d-4bbb-a46c-6fa24026f522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': 'captions_val2014.json', 'train': 'captions_train2014.json'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_json = {f.replace('captions_','').replace('2014.json',''):f for f in os.listdir(data_path) if f.startswith('caption')}\n",
    "captions_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e9c4e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = load_images(captions_json, imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "76e35793-47f5-4ed6-9df6-998f2a78063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_images(df_train, os.path.join(imgs_path,'train'))\n",
    "#plot_images(df_val, os.path.join(imgs_path,'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c06a1eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    RandomCrop(size=(224, 224), padding=None)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = get_transform()\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a176bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageCaptionDataset(transform=transform, \n",
    "                              df=df_train, \n",
    "                              img_path= os.path.join(imgs_path, 'train')\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "78b5b9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.char_to_id('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b5e84036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14])\n",
      "torch.Size([12])\n",
      "torch.Size([13])\n",
      "torch.Size([14])\n",
      "torch.Size([13])\n",
      "torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "for idx, (_, labeltmp) in enumerate(dataset):\n",
    "    ls.append(labeltmp)\n",
    "    print(labeltmp.shape)\n",
    "    if idx>4:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e608bc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  5,  6,  7,  8,  9, 10, 11,  4, 12, 13, 14,  3],\n",
       "        [ 2, 15, 16, 17,  4, 18, 19, 20, 21, 22, 14,  3,  1,  1],\n",
       "        [ 2,  4, 23, 24, 25, 26, 27, 28, 29,  9, 30, 14,  3,  1],\n",
       "        [ 2,  4, 31, 32,  4, 33,  8, 34, 35, 36, 17,  9, 37,  3],\n",
       "        [ 2,  4, 38,  8, 39, 17,  4, 40, 35, 41, 42, 14,  3,  1],\n",
       "        [ 2,  4, 43, 24, 44, 45,  4, 46, 47, 14,  3,  1,  1,  1]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "pad_sequence(ls, padding_value=dataset.char_to_id('<pad>')).T#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "484c9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3f0c1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cbf62103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 15])\n"
     ]
    }
   ],
   "source": [
    "for imgtmp, labeltmp in train_loader:\n",
    "    print(imgtmp.shape)\n",
    "    print(labeltmp.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3fde96-3203-4742-b574-67c3187a76af",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95004864-7f49-42b4-80e3-6a9b60cc0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.batch= nn.BatchNorm1d(embed_size,momentum = 0.01)\n",
    "        self.embed.weight.data.normal_(0., 0.02)\n",
    "        self.embed.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.batch(self.embed(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc72e18-82b1-4f8c-aa55-7595314e63a8",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*4cE8ZvhN7c_xQRgi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0af058ba-85a6-4a50-af88-927b444612b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size= embed_size\n",
    "        self.drop_prob= 0.2\n",
    "        self.vocabulary_size = vocab_size\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size , self.num_layers,batch_first=True)\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size)\n",
    "        self.linear = nn.Linear(hidden_size, self.vocabulary_size)\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)\n",
    "        features = features.unsqueeze(1)\n",
    "        embeddings = torch.cat((features, embeddings[:, :-1,:]), dim=1)\n",
    "        hiddens, c = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254813ee-88c2-44c8-9c16-c0da9a8b3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0edcae61-4bd3-4691-ae8c-11007447878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "transform_train = get_transform(mean=mean, std=std)\n",
    "\n",
    "# Set the minimum word count threshold.\n",
    "vocab_threshold = 6\n",
    "\n",
    "# Specify the batch size.\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5dd3f-d1eb-4f96-9122-20271247113d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df909a2-1591-4090-81c7-19b850056870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "224d2a13-3e59-4843-98ec-97ca686511a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'person', 'doing', 'a', 'trick', 'on', 'a', 'rail', 'while', 'riding', 'a', 'skateboard', '.']\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7680c893-5ae5-4bdb-ba6d-c72ad5415061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b402439-7cb7-4285-85b0-20f07766d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIDE_SIZE = 224\n",
    "MEAN = [0.45, 0.45, 0.45]\n",
    "STD = [0.225, 0.225, 0.225]\n",
    "CROP_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88421cef-a59a-4d95-9922-051b3144f5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
